{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuann3/YOLO-Research-Report/blob/main/yolo_research_biomed_blood_cell.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed17452a",
      "metadata": {
        "id": "ed17452a"
      },
      "source": [
        "# YOLO for Biomedical Image Detection 2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we evaluate the model using Precision, Recall, F1-score, mAP@50, mAP@50-95, and FPS\n",
        "\n",
        "since there is a lot of blood-cells dataset on roboflow with good support, part of the code is from ultralytics offical doc (but we know this is execuse cuz ddl is tmr and i havent write anything yet)"
      ],
      "metadata": {
        "id": "wOrtuHYe48YZ"
      },
      "id": "wOrtuHYe48YZ"
    },
    {
      "cell_type": "markdown",
      "id": "3e9042df",
      "metadata": {
        "id": "3e9042df"
      },
      "source": [
        "## Install YOLO11 via Ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31011949",
      "metadata": {
        "id": "31011949"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()\n",
        "\n",
        "!pip install roboflow\n",
        "!pip install matplotlib seaborn pandas numpy\n",
        "!pip install scikit-learn\n",
        "!pip install supervision\n",
        "\n",
        "HOME = \"/content\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3b2ab70",
      "metadata": {
        "id": "c3b2ab70"
      },
      "source": [
        "## Dataset download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecb8cd8c",
      "metadata": {
        "id": "ecb8cd8c"
      },
      "outputs": [],
      "source": [
        "!mkdir -p {HOME}/datasets\n",
        "%cd {HOME}/datasets\n",
        "\n",
        "!pip install roboflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25d6f086",
      "metadata": {
        "id": "25d6f086"
      },
      "outputs": [],
      "source": [
        "from roboflow import Roboflow\n",
        "\n",
        "# who da fak will put put their api key here\n",
        "# oh is me cuz at this stage i really dont care about it that much\n",
        "rf = Roboflow(api_key=\"SHNYKA5a9WL7F1HrKRNh\")\n",
        "project = rf.workspace(\"team-roboflow\").project(\"blood-cell-detection-1ekwu\")\n",
        "version = project.version(3)\n",
        "dataset = version.download(\"yolov11\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ok lets make sure the this shit is working"
      ],
      "metadata": {
        "id": "QMNRfgaz58tV"
      },
      "id": "QMNRfgaz58tV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7abe6453",
      "metadata": {
        "id": "7abe6453"
      },
      "outputs": [],
      "source": [
        "%cd {dataset.location}\n",
        "!ls\n",
        "!cat data.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16aa3a4c",
      "metadata": {
        "id": "16aa3a4c"
      },
      "source": [
        "# Analysis the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d322d2b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "d322d2b9",
        "outputId": "ed82d1d9-9dda-4487-bebd-de2894bcff3e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-1-be933fffc6f5>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-be933fffc6f5>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ```python\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "import yaml\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cc806c1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "3cc806c1",
        "outputId": "662909eb-15d6-4ee9-bf64-2b6cacafaf38"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-52416346930f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load dataset configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{dataset.location}/data.yaml\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdata_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ],
      "source": [
        "with open(f\"{dataset.location}/data.yaml\", 'r') as file:\n",
        "    data_config = yaml.safe_load(file)\n",
        "\n",
        "print(\"Classes:\", data_config['names'])\n",
        "print(\"Number of classes:\", data_config['nc'])\n",
        "\n",
        "# Analyze class distribution in training set\n",
        "train_labels_dir = os.path.join(dataset.location, \"train\", \"labels\")\n",
        "val_labels_dir = os.path.join(dataset.location, \"valid\", \"labels\")\n",
        "\n",
        "def count_classes(labels_dir, class_count):\n",
        "    for label_file in os.listdir(labels_dir):\n",
        "        if label_file.endswith('.txt'):\n",
        "            with open(os.path.join(labels_dir, label_file), 'r') as f:\n",
        "                for line in f:\n",
        "                    parts = line.strip().split()\n",
        "                    if len(parts) >= 1:\n",
        "                        class_id = int(parts[0])\n",
        "                        class_count[class_id] += 1\n",
        "    return class_count\n",
        "\n",
        "class_count = [0] * data_config['nc']\n",
        "\n",
        "class_count = count_classes(train_labels_dir, class_count)\n",
        "\n",
        "df_classes = pd.DataFrame({\n",
        "    'Class': data_config['names'],\n",
        "    'Count': class_count\n",
        "})\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x='Class', y='Count', data=df_classes)\n",
        "plt.title('Class Distribution in Training Set')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "train_images_dir = os.path.join(dataset.location, \"train\", \"images\")\n",
        "image_sizes = []\n",
        "\n",
        "for img_file in os.listdir(train_images_dir)[:100]:  # limit to 100 images for speed cuz im running out credit and i need my 4090 to play minecraft\n",
        "    if img_file.endswith(('.jpg', '.jpeg', '.png')):\n",
        "        img_path = os.path.join(train_images_dir, img_file)\n",
        "        img = Image.open(img_path)\n",
        "        width, height = img.size\n",
        "        image_sizes.append((width, height))\n",
        "\n",
        "widths, heights = zip(*image_sizes)\n",
        "\n",
        "# i sill got 12 hours left, 2 more ddls, chill\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(widths, heights, alpha=0.5)\n",
        "plt.xlabel('Width (pixels)')\n",
        "plt.ylabel('Height (pixels)')\n",
        "plt.title('Image Dimensions in Training Set')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# calculate average image dimensions\n",
        "avg_width = sum(widths) / len(widths)\n",
        "avg_height = sum(heights) / len(heights)\n",
        "print(f\"Average image dimensions: {avg_width:.1f} x {avg_height:.1f} pixels\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66ef0637",
      "metadata": {
        "id": "66ef0637"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08de352f",
      "metadata": {
        "id": "08de352f"
      },
      "outputs": [],
      "source": [
        "%cd {HOME}\n",
        "\n",
        "# training config since the dataset is big so we just go epochs 50+ above\n",
        "!yolo task=detect mode=train model=yolo11s.pt data={dataset.location}/data.yaml epochs=50 imgsz=640 plots=True patience=15 save_period=10"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04806a7c",
      "metadata": {
        "id": "04806a7c"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "211cfd24",
      "metadata": {
        "id": "211cfd24"
      },
      "outputs": [],
      "source": [
        "%cd {HOME}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02b7debf",
      "metadata": {
        "id": "02b7debf"
      },
      "outputs": [],
      "source": [
        "# Run validation on the test set\n",
        "!yolo task=detect mode=val model={HOME}/runs/detect/train/weights/best.pt data={dataset.location}/data.yaml split=test\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5abe429e",
      "metadata": {
        "id": "5abe429e"
      },
      "outputs": [],
      "source": [
        "detailed evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07573423",
      "metadata": {
        "id": "07573423"
      },
      "outputs": [],
      "source": [
        "```python\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import time\n",
        "from ultralytics import YOLO\n",
        "from pathlib import Path\n",
        "import glob\n",
        "from PIL import Image\n",
        "import supervision as sv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42b09b97",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "42b09b97"
      },
      "outputs": [],
      "source": [
        "# Load the trained model\n",
        "model = YOLO(f\"{HOME}/runs/detect/train/weights/best.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14edd1b2",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "14edd1b2"
      },
      "outputs": [],
      "source": [
        "# Function to parse validation results\n",
        "def parse_results(results_file):\n",
        "    with open(results_file, 'r') as f:\n",
        "        for line in f:\n",
        "            if line.startswith('                 Class'):\n",
        "                header_line = line.strip()\n",
        "            if line.startswith('                   all'):\n",
        "                metrics_line = line.strip()\n",
        "                break\n",
        "\n",
        "    headers = header_line.split()\n",
        "    metrics = metrics_line.split()\n",
        "\n",
        "    class_name = metrics[0]\n",
        "    images = int(metrics[1])\n",
        "    instances = int(metrics[2])\n",
        "    precision = float(metrics[3])\n",
        "    recall = float(metrics[4])\n",
        "    mAP50 = float(metrics[5])\n",
        "    mAP50_95 = float(metrics[6])\n",
        "\n",
        "    return {\n",
        "        'Class': class_name,\n",
        "        'Images': images,\n",
        "        'Instances': instances,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'mAP50': mAP50,\n",
        "        'mAP50-95': mAP50_95\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d2438e6",
      "metadata": {
        "id": "7d2438e6"
      },
      "outputs": [],
      "source": [
        "# Get the path to validation results\n",
        "val_results_file = sorted(Path(f\"{HOME}/runs/detect\").glob(\"val*/results.txt\"))[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a473b14f",
      "metadata": {
        "id": "a473b14f"
      },
      "outputs": [],
      "source": [
        "# Parse validation results\n",
        "overall_metrics = parse_results(val_results_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ffe1971",
      "metadata": {
        "id": "2ffe1971"
      },
      "outputs": [],
      "source": [
        "# Display overall metrics\n",
        "print(\"\\nOverall Metrics:\")\n",
        "print(f\"Precision: {overall_metrics['Precision']:.4f}\")\n",
        "print(f\"Recall: {overall_metrics['Recall']:.4f}\")\n",
        "print(f\"mAP50: {overall_metrics['mAP50']:.4f}\")\n",
        "print(f\"mAP50-95: {overall_metrics['mAP50-95']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "923d6b18",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "923d6b18"
      },
      "outputs": [],
      "source": [
        "# Calculate F1 score\n",
        "f1_score = 2 * (overall_metrics['Precision'] * overall_metrics['Recall']) / (overall_metrics['Precision'] + overall_metrics['Recall'])\n",
        "print(f\"F1 Score: {f1_score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7b38ca8",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "e7b38ca8"
      },
      "outputs": [],
      "source": [
        "# FPS Measurement\n",
        "def measure_fps(model, test_images, num_runs=100):\n",
        "    # Warmup\n",
        "    for _ in range(10):\n",
        "        _ = model(test_images[0])\n",
        "\n",
        "    # Measure inference time\n",
        "    start_time = time.time()\n",
        "    for i in range(min(num_runs, len(test_images))):\n",
        "        _ = model(test_images[i])\n",
        "    end_time = time.time()\n",
        "\n",
        "    elapsed_time = end_time - start_time\n",
        "    fps = num_runs / elapsed_time\n",
        "    return fps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a4bcb06",
      "metadata": {
        "id": "0a4bcb06"
      },
      "outputs": [],
      "source": [
        "# Load some test images for FPS measurement\n",
        "test_images_path = os.path.join(dataset.location, \"test\", \"images\")\n",
        "test_images = [os.path.join(test_images_path, img) for img in os.listdir(test_images_path)\n",
        "               if img.endswith(('.jpg', '.jpeg', '.png'))][:100]  # Limit to 100 images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72560d8b",
      "metadata": {
        "id": "72560d8b"
      },
      "outputs": [],
      "source": [
        "# Measure FPS\n",
        "fps = measure_fps(model, test_images)\n",
        "print(f\"\\nInference Speed: {fps:.2f} FPS\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a78730ee",
      "metadata": {
        "id": "a78730ee"
      },
      "outputs": [],
      "source": [
        "# Create a metrics table\n",
        "metrics_table = pd.DataFrame({\n",
        "    'Metric': ['Precision', 'Recall', 'F1 Score', 'mAP50', 'mAP50-95', 'FPS'],\n",
        "    'Value': [\n",
        "        overall_metrics['Precision'],\n",
        "        overall_metrics['Recall'],\n",
        "        f1_score,\n",
        "        overall_metrics['mAP50'],\n",
        "        overall_metrics['mAP50-95'],\n",
        "        fps\n",
        "    ]\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac533776",
      "metadata": {
        "id": "ac533776"
      },
      "outputs": [],
      "source": [
        "print(\"\\nMetrics Summary:\")\n",
        "print(metrics_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6280539",
      "metadata": {
        "id": "e6280539"
      },
      "outputs": [],
      "source": [
        "# Save metrics to CSV for inclusion in the paper\n",
        "metrics_table.to_csv(f\"{HOME}/biomedical_detection_metrics.csv\", index=False)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7aee5a1d",
      "metadata": {
        "id": "7aee5a1d"
      },
      "source": [
        "# Visualize Evaluation Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b877a73",
      "metadata": {
        "id": "9b877a73"
      },
      "outputs": [],
      "source": [
        "```python\n",
        "# Create a bar plot of the main metrics\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x='Metric', y='Value', data=metrics_table[metrics_table['Metric'] != 'FPS'])\n",
        "plt.title('YOLOv11 Performance Metrics on Biomedical Dataset')\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.savefig(f\"{HOME}/metrics_barplot.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47de048d",
      "metadata": {
        "id": "47de048d"
      },
      "outputs": [],
      "source": [
        "# Plot precision-recall curve (if data is available from validation)\n",
        "try:\n",
        "    pr_curve = pd.read_csv(f\"{HOME}/runs/detect/train/results.csv\")\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(pr_curve['               recall'], pr_curve['             precision'], marker='o', linewidth=2)\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.grid(True)\n",
        "    plt.savefig(f\"{HOME}/precision_recall_curve.png\")\n",
        "    plt.show()\n",
        "except:\n",
        "    print(\"Could not load precision-recall data from results.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68a02489",
      "metadata": {
        "id": "68a02489"
      },
      "outputs": [],
      "source": [
        "# Visualize class-wise metrics from validation results\n",
        "val_data = []\n",
        "with open(val_results_file, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        parts = line.strip().split()\n",
        "        if len(parts) >= 7 and parts[0] != 'Class' and parts[0] != 'all':\n",
        "            try:\n",
        "                class_name = parts[0]\n",
        "                precision = float(parts[3])\n",
        "                recall = float(parts[4])\n",
        "                mAP50 = float(parts[5])\n",
        "                mAP50_95 = float(parts[6])\n",
        "                val_data.append({\n",
        "                    'Class': class_name,\n",
        "                    'Precision': precision,\n",
        "                    'Recall': recall,\n",
        "                    'mAP50': mAP50,\n",
        "                    'mAP50-95': mAP50_95\n",
        "                })\n",
        "            except:\n",
        "                continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77187189",
      "metadata": {
        "id": "77187189"
      },
      "outputs": [],
      "source": [
        "if val_data:\n",
        "    df_val = pd.DataFrame(val_data)\n",
        "\n",
        "    # Plot class-wise metrics\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    df_melted = pd.melt(df_val, id_vars=['Class'], var_name='Metric', value_name='Value')\n",
        "    sns.barplot(x='Class', y='Value', hue='Metric', data=df_melted)\n",
        "    plt.title('Class-wise Performance Metrics')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.ylim(0, 1)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{HOME}/class_metrics.png\")\n",
        "    plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09c40cfc",
      "metadata": {
        "id": "09c40cfc"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9802875c",
      "metadata": {
        "id": "9802875c"
      },
      "outputs": [],
      "source": [
        "```python\n",
        "%cd {HOME}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebf9a3c7",
      "metadata": {
        "id": "ebf9a3c7"
      },
      "outputs": [],
      "source": [
        "# Run inference on test images\n",
        "!yolo task=detect mode=predict model={HOME}/runs/detect/train/weights/best.pt conf=0.25 source={dataset.location}/test/images save=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e98c68d4",
      "metadata": {
        "id": "e98c68d4"
      },
      "outputs": [],
      "source": [
        "# Visualize some results\n",
        "import glob\n",
        "from IPython.display import Image as IPyImage, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c73293d",
      "metadata": {
        "id": "7c73293d"
      },
      "outputs": [],
      "source": [
        "latest_folder = max(glob.glob(f'{HOME}/runs/detect/predict*/'), key=os.path.getmtime)\n",
        "print(f\"Visualization results saved to: {latest_folder}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e9144f2",
      "metadata": {
        "id": "6e9144f2"
      },
      "outputs": [],
      "source": [
        "# Show first 5 predictions\n",
        "for img in sorted(glob.glob(f'{latest_folder}/*.jpg'))[:5]:\n",
        "    display(IPyImage(filename=img, width=600))\n",
        "    print(\"\\n\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccaf1281",
      "metadata": {
        "id": "ccaf1281"
      },
      "source": [
        "# Advanced Analysis for Research Paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42464bc1",
      "metadata": {
        "id": "42464bc1"
      },
      "outputs": [],
      "source": [
        " some additional code to generate visualizations and analyses for our research paper:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90d6b59b",
      "metadata": {
        "id": "90d6b59b"
      },
      "outputs": [],
      "source": [
        "```python\n",
        "# Confusion Matrix Analysis\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image as IPyImage, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc04b472",
      "metadata": {
        "id": "fc04b472"
      },
      "outputs": [],
      "source": [
        "# Display confusion matrix from training run\n",
        "confusion_matrix_path = f\"{HOME}/runs/detect/train/confusion_matrix.png\"\n",
        "if os.path.exists(confusion_matrix_path):\n",
        "    display(IPyImage(filename=confusion_matrix_path, width=800))\n",
        "    print(\"Confusion Matrix from Training\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5f64875",
      "metadata": {
        "id": "a5f64875"
      },
      "outputs": [],
      "source": [
        "# Calculate and visualize additional metrics for the paper\n",
        "if val_data:\n",
        "    df_val = pd.DataFrame(val_data)\n",
        "\n",
        "    # Calculate F1 scores for each class\n",
        "    df_val['F1-Score'] = 2 * (df_val['Precision'] * df_val['Recall']) / (df_val['Precision'] + df_val['Recall'])\n",
        "\n",
        "    # Radar chart for multi-dimensional performance metrics\n",
        "    from matplotlib.path import Path\n",
        "    from matplotlib.splines import Spline\n",
        "    from matplotlib.patches import Circle, RegularPolygon\n",
        "    from matplotlib.projections import register_projection\n",
        "    from matplotlib.projections.polar import PolarAxes\n",
        "    from matplotlib.transforms import Affine2D\n",
        "\n",
        "\n",
        "   # not quite sure what im doing here, but it works\n",
        "    def radar_factory(num_vars, frame='circle'):\n",
        "        theta = np.linspace(0, 2*np.pi, num_vars, endpoint=False)\n",
        "        class RadarAxes(PolarAxes):\n",
        "            name = 'radar'\n",
        "\n",
        "            def __init__(self, *args, **kwargs):\n",
        "                super().__init__(*args, **kwargs)\n",
        "                self.set_theta_zero_location('N')\n",
        "\n",
        "            def fill(self, *args, closed=True, **kwargs):\n",
        "                return super().fill(self.theta, args[0], closed=closed, **kwargs)\n",
        "\n",
        "            def plot(self, *args, **kwargs):\n",
        "                return super().plot(self.theta, args[0], **kwargs)\n",
        "\n",
        "            def set_varlabels(self, labels):\n",
        "                self.set_thetagrids(np.degrees(self.theta), labels)\n",
        "\n",
        "            def _gen_axes_patch(self):\n",
        "                if frame == 'circle':\n",
        "                    return Circle((0.5, 0.5), 0.5)\n",
        "                elif frame == 'polygon':\n",
        "                    return RegularPolygon((0.5, 0.5), num_vars, radius=0.5, edgecolor=\"k\")\n",
        "                else:\n",
        "                    raise ValueError(\"Unknown value for 'frame': %s\" % frame)\n",
        "\n",
        "            def _gen_axes_spines(self):\n",
        "                if frame == 'circle':\n",
        "                    return super()._gen_axes_spines()\n",
        "                elif frame == 'polygon':\n",
        "                    spine_dict = {}\n",
        "                    for i in range(num_vars):\n",
        "                        spine = Spline(self.Axes.path, self._path_transform)\n",
        "                        spine.set_transform(self.transAxes)\n",
        "                        spine_dict[self.Axes.get_side(i)] = spine\n",
        "                    return spine_dict\n",
        "                else:\n",
        "                    raise ValueError(\"Unknown value for 'frame': %s\" % frame)\n",
        "\n",
        "        register_projection(RadarAxes)\n",
        "        return theta\n",
        "\n",
        "    # Get top 5 classes by mAP50\n",
        "    top_classes = df_val.sort_values('mAP50', ascending=False).head(5)\n",
        "\n",
        "    # Prepare data for radar chart\n",
        "    metrics = ['Precision', 'Recall', 'F1-Score', 'mAP50', 'mAP50-95']\n",
        "    data = []\n",
        "    for _, row in top_classes.iterrows():\n",
        "        values = [row[metric] for metric in metrics]\n",
        "        data.append(values)\n",
        "\n",
        "    # Create radar chart\n",
        "    theta = radar_factory(len(metrics), frame='polygon')\n",
        "    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(projection='radar'))\n",
        "\n",
        "    colors = ['b', 'g', 'r', 'c', 'm']\n",
        "    for i, (class_data, color) in enumerate(zip(data, colors)):\n",
        "        ax.plot(class_data, color=color)\n",
        "        ax.fill(class_data, alpha=0.1, color=color)\n",
        "\n",
        "    ax.set_varlabels(metrics)\n",
        "    ax.set_ylim(0, 1)\n",
        "    plt.legend(top_classes['Class'].values, loc='upper right')\n",
        "    plt.title('Multi-dimensional Performance Analysis of Top 5 Classes')\n",
        "    plt.savefig(f\"{HOME}/radar_metrics.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    # Create a heatmap of metrics by class\n",
        "    plt.figure(figsize=(12, len(df_val) * 0.5))\n",
        "    sns.heatmap(df_val[metrics].values,\n",
        "                annot=True,\n",
        "                fmt=\".3f\",\n",
        "                cmap=\"YlGnBu\",\n",
        "                yticklabels=df_val['Class'],\n",
        "                xticklabels=metrics,\n",
        "                vmin=0, vmax=1)\n",
        "    plt.title('Performance Metrics Heatmap by Class')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{HOME}/metrics_heatmap.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "410e0db4",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "410e0db4"
      },
      "source": [
        "# Results Analysis and Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4957e03c",
      "metadata": {
        "id": "4957e03c"
      },
      "outputs": [],
      "source": [
        "```python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c46da8ba",
      "metadata": {
        "id": "c46da8ba"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\\n---------- RESULTS SUMMARY ----------\")\n",
        "print(f\"\\nOverall Performance Metrics:\")\n",
        "print(f\"Precision: {overall_metrics['Precision']:.4f}\")\n",
        "print(f\"Recall: {overall_metrics['Recall']:.4f}\")\n",
        "print(f\"F1 Score: {f1_score:.4f}\")\n",
        "print(f\"mAP@50: {overall_metrics['mAP50']:.4f}\")\n",
        "print(f\"mAP@50-95: {overall_metrics['mAP50-95']:.4f}\")\n",
        "print(f\"Inference Speed: {fps:.2f} FPS\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25e549dd",
      "metadata": {
        "id": "25e549dd"
      },
      "outputs": [],
      "source": [
        "# Interpretation of results\n",
        "print(\"\\nInterpretation of Results:\")\n",
        "print(\"\"\"\n",
        "1. **Overall Performance**: The YOLOv11 model achieved [precision] precision, [recall] recall, and an F1 score of [f1_score], indicating [strong/moderate/weak] performance on the biomedical dataset.\n",
        "\n",
        "2. **Detection Accuracy**: The mAP@50 of [mAP50] and mAP@50-95 of [mAP50-95] demonstrate that the model performs [well/acceptably/poorly] across different IoU thresholds, suggesting [robust/variable] detection capabilities.\n",
        "\n",
        "3. **Real-time Capabilities**: With an inference speed of [fps] FPS, the model [is/is not] suitable for real-time applications in biomedical image analysis.\n",
        "\n",
        "4. **Class-wise Performance**: Analysis of class-wise metrics reveals that [best_classes] showed the highest detection accuracy, while [worst_classes] had lower performance. This suggests that the model [is more effective at/struggles with] detecting certain types of biomedical objects.\n",
        "\n",
        "5. **Comparison to Literature**: These results [exceed/are comparable to/fall short of] the current state-of-the-art in biomedical object detection, which typically reports mAP50 values between [literature_range].\n",
        "\n",
        "6. **Practical Implications**: The evaluation metrics indicate that YOLOv11 [is/is not] a viable option for biomedical image analysis tasks, particularly for [specific applications].\n",
        "\"\"\".replace('[precision]', f\"{overall_metrics['Precision']:.4f}\")\n",
        "   .replace('[recall]', f\"{overall_metrics['Recall']:.4f}\")\n",
        "   .replace('[f1_score]', f\"{f1_score:.4f}\")\n",
        "   .replace('[mAP50]', f\"{overall_metrics['mAP50']:.4f}\")\n",
        "   .replace('[mAP50-95]', f\"{overall_metrics['mAP50-95']:.4f}\")\n",
        "   .replace('[fps]', f\"{fps:.2f}\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "315e6462",
      "metadata": {
        "id": "315e6462"
      },
      "outputs": [],
      "source": [
        "# Generate JSON results for programmatic use\n",
        "results_json = {\n",
        "    \"model\": \"YOLOv11s\",\n",
        "    \"dataset\": \"Blood Cell Detection\",\n",
        "    \"metrics\": {\n",
        "        \"precision\": overall_metrics['Precision'],\n",
        "        \"recall\": overall_metrics['Recall'],\n",
        "        \"f1_score\": f1_score,\n",
        "        \"mAP50\": overall_metrics['mAP50'],\n",
        "        \"mAP50-95\": overall_metrics['mAP50-95'],\n",
        "        \"fps\": fps\n",
        "    },\n",
        "    \"training_details\": {\n",
        "        \"epochs\": 50,\n",
        "        \"image_size\": 640,\n",
        "        \"optimizer\": \"Adam\",\n",
        "        \"batch_size\": 16\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6028eb89",
      "metadata": {
        "id": "6028eb89"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open(f\"{HOME}/results_summary.json\", 'w') as f:\n",
        "    json.dump(results_json, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f2c2b48",
      "metadata": {
        "id": "1f2c2b48"
      },
      "outputs": [],
      "source": [
        "print(f\"\\nResults summary saved to {HOME}/results_summary.json\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8cd134b",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "a8cd134b"
      },
      "source": [
        "# Comparison with Different Versions of the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa4f8abf",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "fa4f8abf"
      },
      "outputs": [],
      "source": [
        "```python\n",
        "# Function to train and evaluate on different dataset versions\n",
        "def train_and_evaluate(version_number):\n",
        "    print(f\"\\n\\n---------- EVALUATING DATASET VERSION {version_number} ----------\")\n",
        "\n",
        "    # Download dataset\n",
        "    %cd {HOME}/datasets\n",
        "\n",
        "    from roboflow import Roboflow\n",
        "    rf = Roboflow(api_key=\"SHNYKA5a9WL7F1HrKRNh\")\n",
        "    project = rf.workspace(\"team-roboflow\").project(\"blood-cell-detection-1ekwu\")\n",
        "    version = project.version(version_number)\n",
        "    dataset = version.download(\"yolov11\")\n",
        "\n",
        "    # Train model (fewer epochs for comparison)\n",
        "    %cd {HOME}\n",
        "    training_output = !yolo task=detect mode=train model=yolo11s.pt data={dataset.location}/data.yaml epochs=20 imgsz=640 plots=True patience=5 name=version_{version_number}\n",
        "\n",
        "    # Evaluate model\n",
        "    val_output = !yolo task=detect mode=val model={HOME}/runs/detect/version_{version_number}/weights/best.pt data={dataset.location}/data.yaml\n",
        "\n",
        "    # Extract metrics\n",
        "    metrics = {}\n",
        "    for line in val_output:\n",
        "        if line.startswith('                   all'):\n",
        "            parts = line.strip().split()\n",
        "            metrics = {\n",
        "                'Precision': float(parts[3]),\n",
        "                'Recall': float(parts[4]),\n",
        "                'mAP50': float(parts[5]),\n",
        "                'mAP50-95': float(parts[6])\n",
        "            }\n",
        "            break\n",
        "\n",
        "    # Calculate F1\n",
        "    if 'Precision' in metrics and 'Recall' in metrics:\n",
        "        metrics['F1-Score'] = 2 * (metrics['Precision'] * metrics['Recall']) / (metrics['Precision'] + metrics['Recall'])\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c731055",
      "metadata": {
        "id": "4c731055"
      },
      "outputs": [],
      "source": [
        "# Compare versions 1 and 2 (version 3 was already evaluated in the main flow)\n",
        "version1_metrics = train_and_evaluate(1)\n",
        "version2_metrics = train_and_evaluate(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f486ced1",
      "metadata": {
        "id": "f486ced1"
      },
      "outputs": [],
      "source": [
        "# Get version 3 metrics from earlier evaluation\n",
        "version3_metrics = {\n",
        "    'Precision': overall_metrics['Precision'],\n",
        "    'Recall': overall_metrics['Recall'],\n",
        "    'mAP50': overall_metrics['mAP50'],\n",
        "    'mAP50-95': overall_metrics['mAP50-95'],\n",
        "    'F1-Score': f1_score\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41690937",
      "metadata": {
        "id": "41690937"
      },
      "outputs": [],
      "source": [
        "# Create comparison table\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Metric': ['Precision', 'Recall', 'F1-Score', 'mAP50', 'mAP50-95'],\n",
        "    'Version 1': [version1_metrics.get('Precision', np.nan),\n",
        "                 version1_metrics.get('Recall', np.nan),\n",
        "                 version1_metrics.get('F1-Score', np.nan),\n",
        "                 version1_metrics.get('mAP50', np.nan),\n",
        "                 version1_metrics.get('mAP50-95', np.nan)],\n",
        "    'Version 2': [version2_metrics.get('Precision', np.nan),\n",
        "                 version2_metrics.get('Recall', np.nan),\n",
        "                 version2_metrics.get('F1-Score', np.nan),\n",
        "                 version2_metrics.get('mAP50', np.nan),\n",
        "                 version2_metrics.get('mAP50-95', np.nan)],\n",
        "    'Version 3': [version3_metrics.get('Precision', np.nan),\n",
        "                 version3_metrics.get('Recall', np.nan),\n",
        "                 version3_metrics.get('F1-Score', np.nan),\n",
        "                 version3_metrics.get('mAP50', np.nan),\n",
        "                 version3_metrics.get('mAP50-95', np.nan)]\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94d0ba80",
      "metadata": {
        "id": "94d0ba80"
      },
      "outputs": [],
      "source": [
        "print(\"\\nComparison of Dataset Versions:\")\n",
        "print(comparison_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "166a6170",
      "metadata": {
        "id": "166a6170"
      },
      "outputs": [],
      "source": [
        "# Create a visualization of the comparison\n",
        "plt.figure(figsize=(14, 8))\n",
        "comparison_melted = pd.melt(comparison_df, id_vars=['Metric'], var_name='Dataset Version', value_name='Value')\n",
        "sns.barplot(x='Metric', y='Value', hue='Dataset Version', data=comparison_melted)\n",
        "plt.title('Performance Comparison Across Dataset Versions')\n",
        "plt.ylim(0, 1)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.savefig(f\"{HOME}/version_comparison.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36851e7b",
      "metadata": {
        "id": "36851e7b"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "331269eb",
      "metadata": {
        "id": "331269eb"
      },
      "outputs": [],
      "source": [
        "This notebook demonstrates the entire process of training a YOLOv11 model on a biomedical dataset and evaluating its performance using a comprehensive set of metrics including Precision, Recall, F1-score, mAP@50, mAP@50-95, and FPS. These metrics provide a thorough basis for the experimental evaluation section of your research paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b183fce0",
      "metadata": {
        "id": "b183fce0"
      },
      "outputs": [],
      "source": [
        "The analysis includes:\n",
        "1. Training a custom YOLOv11 model on biomedical images\n",
        "2. Evaluating the model with multiple performance metrics\n",
        "3. Visualizing the results through various plots\n",
        "4. Comparing performance across different versions of the dataset\n",
        "5. Generating artifacts suitable for inclusion in a research paper"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}